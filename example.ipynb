{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load pre-trained overall sentiment analysis model and tokenizer\n",
    "overall_model_name = (\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\"  # Overall sentiment model\n",
    ")\n",
    "overall_tokenizer = AutoTokenizer.from_pretrained(overall_model_name)\n",
    "overall_model = AutoModelForSequenceClassification.from_pretrained(overall_model_name)\n",
    "\n",
    "# Load pre-trained ABSA model and tokenizer\n",
    "absa_model_name = \"absa/classifier-rest-0.2\"  # Aspect-based sentiment analysis model\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(absa_model_name)\n",
    "absa_model = AutoModelForTokenClassification.from_pretrained(absa_model_name)\n",
    "\n",
    "\n",
    "# Function to preprocess reviews for overall sentiment analysis\n",
    "def preprocess_for_overall_sentiment(review_text, max_length=512):\n",
    "    tokenized_input = overall_tokenizer(\n",
    "        review_text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "# Function to preprocess reviews for aspect-based sentiment analysis\n",
    "def preprocess_for_absa(review_text, max_length=512):\n",
    "    tokenized_input = absa_tokenizer(\n",
    "        review_text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "# Predict overall sentiment for a review\n",
    "def predict_overall_sentiment(tokenized_input):\n",
    "    with torch.no_grad():\n",
    "        outputs = overall_model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(\n",
    "        logits, dim=1\n",
    "    ).item()  # Class (e.g., 0=negative, 1=neutral, 2=positive)\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "# Predict aspects and their sentiment from the review using ABSA\n",
    "def predict_aspects_and_sentiments(tokenized_input):\n",
    "    with torch.no_grad():\n",
    "        outputs = absa_model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "    predicted_aspects = torch.argmax(\n",
    "        logits, dim=2\n",
    "    )  # Get the aspect sentiment classification\n",
    "    return predicted_aspects\n",
    "\n",
    "\n",
    "# Load review data from JSON file\n",
    "def load_reviews_from_json(json_file):\n",
    "    with open(json_file, \"r\", encode=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"reviews\"]\n",
    "\n",
    "\n",
    "# Process reviews for both overall and aspect-based sentiment\n",
    "def process_reviews(json_file):\n",
    "    reviews_data = load_reviews_from_json(json_file)\n",
    "    results = []\n",
    "\n",
    "    for review in reviews_data:\n",
    "        review_text = review.get(\"content\")  # Adjust based on your JSON structure\n",
    "\n",
    "        # Overall sentiment analysis\n",
    "        tokenized_review_overall = preprocess_for_overall_sentiment(review_text)\n",
    "        overall_sentiment = predict_overall_sentiment(tokenized_review_overall)\n",
    "\n",
    "        # Aspect-based sentiment analysis\n",
    "        tokenized_review_absa = preprocess_for_absa(review_text)\n",
    "        aspects_sentiments = predict_aspects_and_sentiments(tokenized_review_absa)\n",
    "\n",
    "        # Store the review text, overall sentiment, and aspect sentiments\n",
    "        results.append(\n",
    "            {\n",
    "                \"reviewId\": review.get(\"reviewId\"),\n",
    "                \"reviewText\": review_text,\n",
    "                \"overallSentiment\": overall_sentiment,  # Overall sentiment: 0=negative, 1=neutral, 2=positive\n",
    "                \"aspectsSentiments\": aspects_sentiments,  # Aspect-sentiment pairs\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame and save to CSV\n",
    "def save_to_csv(results, output_csv):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Main function to load, process, and save results\n",
    "def main(json_file, output_csv):\n",
    "    results = process_reviews(json_file)\n",
    "    save_to_csv(results, output_csv)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "json_file = \"Habitica:GamifyYourTasks.json\"  # Path to your JSON file containing reviews\n",
    "output_csv = \"processed_reviews_with_overall_and_aspects.csv\"  # Output CSV file path\n",
    "\n",
    "main(json_file, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import regex as re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic cleaning of text while preserving the multilingual characters\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.UNICODE)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load reviews data from a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentimental_analysis(data, app_name):\n",
    "    # Preprocess reviews and perform sentiment analysis\n",
    "    results = []\n",
    "    for review in data[\"reviews\"]:\n",
    "        review_id = review.get(\"reviewId\")\n",
    "        content = review.get(\"content\")\n",
    "\n",
    "        if content:  # Only analyze reviews that have content\n",
    "            content = clean_text(content)\n",
    "            sentiment = sentiment_analyzer(content)[0]  # Perform sentiment analysis\n",
    "            sentiment_score = sentiment[\n",
    "                \"label\"\n",
    "            ]  # The model gives a label like \"1 star\", \"2 stars\", etc.\n",
    "\n",
    "            # Store the result as a dictionary\n",
    "            results.append(\n",
    "                {\n",
    "                    \"reviewId\": review_id,\n",
    "                    \"content\": content,\n",
    "                    \"sentimentScore\": sentiment_score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f\"data/processed_data/play_store/{app_name}.csv\", index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Sentiment analysis results for {app_name} saved to 'reviews_sentiment.csv'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis results for HabitShare-HabitTracker saved to 'reviews_sentiment.csv'.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "directory = Path(\"data/raw_data/playstore\")\n",
    "for file_path in directory.iterdir():\n",
    "    if file_path.is_file():\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        perform_sentimental_analysis(data, file_path.stem)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Processing HabitShare-HabitTracker...\n",
      "\n",
      "Processing Dreamfora:AIGoalSetting...\n",
      "\n",
      "Processing Sectograph.Day&Timeplanner...\n",
      "\n",
      "Processing NCalendar-Simpleplanner...\n",
      "\n",
      "Processing Productive-Habittracker...\n",
      "\n",
      "Processing HabitNowDailyRoutinePlanner...\n",
      "\n",
      "Processing TickTick:ToDoList&Calendar...\n",
      "\n",
      "Processing Engross:FocusTimer&To-Do...\n",
      "\n",
      "Processing SaveMyTime-TimeTracker...\n",
      "\n",
      "Processing Habitica:GamifyYourTasks...\n",
      "\n",
      "Processing TheFor:HabitTracker...\n",
      "\n",
      "Processing Prosper-DailyPlannerTodo...\n",
      "\n",
      "Processing WaterDo:ToDoList&Schedule...\n",
      "\n",
      "Processing MinimalistPomodoroTimer...\n",
      "\n",
      "Processing Gratitude:Self-CareJournal...\n",
      "\n",
      "Processing Forest:FocusforProductivity...\n",
      "\n",
      "Processing Me+DailyRoutinePlanner...\n",
      "\n",
      "Processing MoodTracker:Self-CareHabits...\n",
      "\n",
      "Processing FabulousDailyRoutinePlanner...\n",
      "\n",
      "Processing Habitify:DailyHabitTracker...\n",
      "\n",
      "Processing ColorNoteNotepadNotesTodo...\n",
      "Cleaning reviews for ColorNoteNotepadNotesTodo...\n",
      "Processing reviews for ColorNoteNotepadNotesTodo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 13152/14796 [7:55:42<36:28,  1.33s/it]       "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import regex as re\n",
    "import emoji\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length=512):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic cleaning of text while preserving the multilingual characters\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.UNICODE)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_reviews_batch(reviews):\n",
    "    \"\"\"Clean a batch of reviews in parallel\"\"\"\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        cleaned_reviews = pool.map(clean_text, reviews)\n",
    "    return cleaned_reviews\n",
    "\n",
    "\n",
    "def collate_batch(batch, tokenizer, max_length=512):\n",
    "    \"\"\"Collate batch of reviews into tensor format\"\"\"\n",
    "    return tokenizer(\n",
    "        batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "def process_batch(batch, model, device):\n",
    "    \"\"\"Process a batch of reviews and return sentiment scores\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(device) for k, v in batch.items()})\n",
    "        predictions = torch.softmax(outputs.logits, dim=1)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "\n",
    "def perform_sentiment_analysis(data, app_name, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"Perform sentiment analysis on reviews in batches\"\"\"\n",
    "    # Extract review content and IDs\n",
    "    reviews = []\n",
    "    review_ids = []\n",
    "    for review in data[\"reviews\"]:\n",
    "        if review.get(\"content\"):\n",
    "            reviews.append(review[\"content\"])\n",
    "            review_ids.append(review[\"reviewId\"])\n",
    "\n",
    "    if not reviews:\n",
    "        print(f\"No reviews found for {app_name}\")\n",
    "        return\n",
    "\n",
    "    # Clean reviews in parallel\n",
    "    print(f\"Cleaning reviews for {app_name}...\")\n",
    "    cleaned_reviews = clean_reviews_batch(reviews)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ReviewDataset(cleaned_reviews, tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(collate_batch, tokenizer=tokenizer),\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Process batches\n",
    "    all_predictions = []\n",
    "    print(f\"Processing reviews for {app_name}...\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        predictions = process_batch(batch, model, device)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Convert predictions to sentiment scores (1-5)\n",
    "    sentiment_scores = np.argmax(all_predictions, axis=1) + 1\n",
    "\n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"reviewId\": review_ids,\n",
    "            \"content\": cleaned_reviews,\n",
    "            \"sentimentScore\": sentiment_scores,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    output_path = Path(f\"data/processed_data/play_store/{app_name}.csv\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Sentiment analysis results for {app_name} saved to {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Process files\n",
    "    directory = Path(\"data/raw_data/playstore\")\n",
    "    for file_path in directory.iterdir():\n",
    "        if file_path.is_file():\n",
    "            print(f\"\\nProcessing {file_path.stem}...\")\n",
    "            if Path(f'data/processed_data/play_store/{file_path.stem}.csv').exists():\n",
    "                continue\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                perform_sentiment_analysis(\n",
    "                    data, file_path.stem, model, tokenizer, device\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.stem}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to organize reviews by sentiment...\n",
      "\n",
      "Processing Sectograph.Day&Timeplanner...\n",
      "Saved 466 1-star reviews to organized_data/Sectograph.Day&Timeplanner/1_star_reviews.csv\n",
      "Saved 346 2-star reviews to organized_data/Sectograph.Day&Timeplanner/2_star_reviews.csv\n",
      "Saved 817 3-star reviews to organized_data/Sectograph.Day&Timeplanner/3_star_reviews.csv\n",
      "Saved 2371 4-star reviews to organized_data/Sectograph.Day&Timeplanner/4_star_reviews.csv\n",
      "Saved 5026 5-star reviews to organized_data/Sectograph.Day&Timeplanner/5_star_reviews.csv\n",
      "\n",
      "Processing TheFor:HabitTracker...\n",
      "Saved 23 1-star reviews to organized_data/TheFor:HabitTracker/1_star_reviews.csv\n",
      "Saved 21 2-star reviews to organized_data/TheFor:HabitTracker/2_star_reviews.csv\n",
      "Saved 51 3-star reviews to organized_data/TheFor:HabitTracker/3_star_reviews.csv\n",
      "Saved 101 4-star reviews to organized_data/TheFor:HabitTracker/4_star_reviews.csv\n",
      "Saved 238 5-star reviews to organized_data/TheFor:HabitTracker/5_star_reviews.csv\n",
      "\n",
      "Processing WaterDo:ToDoList&Schedule...\n",
      "Saved 133 1-star reviews to organized_data/WaterDo:ToDoList&Schedule/1_star_reviews.csv\n",
      "Saved 70 2-star reviews to organized_data/WaterDo:ToDoList&Schedule/2_star_reviews.csv\n",
      "Saved 149 3-star reviews to organized_data/WaterDo:ToDoList&Schedule/3_star_reviews.csv\n",
      "Saved 397 4-star reviews to organized_data/WaterDo:ToDoList&Schedule/4_star_reviews.csv\n",
      "Saved 574 5-star reviews to organized_data/WaterDo:ToDoList&Schedule/5_star_reviews.csv\n",
      "\n",
      "Processing HabitShare-HabitTracker...\n",
      "Saved 33 1-star reviews to organized_data/HabitShare-HabitTracker/1_star_reviews.csv\n",
      "Saved 28 2-star reviews to organized_data/HabitShare-HabitTracker/2_star_reviews.csv\n",
      "Saved 69 3-star reviews to organized_data/HabitShare-HabitTracker/3_star_reviews.csv\n",
      "Saved 130 4-star reviews to organized_data/HabitShare-HabitTracker/4_star_reviews.csv\n",
      "Saved 270 5-star reviews to organized_data/HabitShare-HabitTracker/5_star_reviews.csv\n",
      "\n",
      "Processing Dreamfora:AIGoalSetting...\n",
      "Saved 269 1-star reviews to organized_data/Dreamfora:AIGoalSetting/1_star_reviews.csv\n",
      "Saved 163 2-star reviews to organized_data/Dreamfora:AIGoalSetting/2_star_reviews.csv\n",
      "Saved 298 3-star reviews to organized_data/Dreamfora:AIGoalSetting/3_star_reviews.csv\n",
      "Saved 780 4-star reviews to organized_data/Dreamfora:AIGoalSetting/4_star_reviews.csv\n",
      "Saved 2061 5-star reviews to organized_data/Dreamfora:AIGoalSetting/5_star_reviews.csv\n",
      "\n",
      "Processing HabitNowDailyRoutinePlanner...\n",
      "Saved 317 1-star reviews to organized_data/HabitNowDailyRoutinePlanner/1_star_reviews.csv\n",
      "Saved 223 2-star reviews to organized_data/HabitNowDailyRoutinePlanner/2_star_reviews.csv\n",
      "Saved 667 3-star reviews to organized_data/HabitNowDailyRoutinePlanner/3_star_reviews.csv\n",
      "Saved 2174 4-star reviews to organized_data/HabitNowDailyRoutinePlanner/4_star_reviews.csv\n",
      "Saved 5863 5-star reviews to organized_data/HabitNowDailyRoutinePlanner/5_star_reviews.csv\n",
      "\n",
      "Processing SaveMyTime-TimeTracker...\n",
      "Saved 140 1-star reviews to organized_data/SaveMyTime-TimeTracker/1_star_reviews.csv\n",
      "Saved 116 2-star reviews to organized_data/SaveMyTime-TimeTracker/2_star_reviews.csv\n",
      "Saved 267 3-star reviews to organized_data/SaveMyTime-TimeTracker/3_star_reviews.csv\n",
      "Saved 696 4-star reviews to organized_data/SaveMyTime-TimeTracker/4_star_reviews.csv\n",
      "Saved 1422 5-star reviews to organized_data/SaveMyTime-TimeTracker/5_star_reviews.csv\n",
      "\n",
      "Processing Prosper-DailyPlannerTodo...\n",
      "Saved 39 1-star reviews to organized_data/Prosper-DailyPlannerTodo/1_star_reviews.csv\n",
      "Saved 24 2-star reviews to organized_data/Prosper-DailyPlannerTodo/2_star_reviews.csv\n",
      "Saved 62 3-star reviews to organized_data/Prosper-DailyPlannerTodo/3_star_reviews.csv\n",
      "Saved 170 4-star reviews to organized_data/Prosper-DailyPlannerTodo/4_star_reviews.csv\n",
      "Saved 518 5-star reviews to organized_data/Prosper-DailyPlannerTodo/5_star_reviews.csv\n",
      "\n",
      "Processing TickTick:ToDoList&Calendar...\n",
      "Saved 1460 1-star reviews to organized_data/TickTick:ToDoList&Calendar/1_star_reviews.csv\n",
      "Saved 897 2-star reviews to organized_data/TickTick:ToDoList&Calendar/2_star_reviews.csv\n",
      "Saved 1719 3-star reviews to organized_data/TickTick:ToDoList&Calendar/3_star_reviews.csv\n",
      "Saved 5027 4-star reviews to organized_data/TickTick:ToDoList&Calendar/4_star_reviews.csv\n",
      "Saved 15221 5-star reviews to organized_data/TickTick:ToDoList&Calendar/5_star_reviews.csv\n",
      "\n",
      "Processing FabulousDailyRoutinePlanner...\n",
      "Saved 9281 1-star reviews to organized_data/FabulousDailyRoutinePlanner/1_star_reviews.csv\n",
      "Saved 4071 2-star reviews to organized_data/FabulousDailyRoutinePlanner/2_star_reviews.csv\n",
      "Saved 5758 3-star reviews to organized_data/FabulousDailyRoutinePlanner/3_star_reviews.csv\n",
      "Saved 17923 4-star reviews to organized_data/FabulousDailyRoutinePlanner/4_star_reviews.csv\n",
      "Saved 53960 5-star reviews to organized_data/FabulousDailyRoutinePlanner/5_star_reviews.csv\n",
      "\n",
      "Processing Forest:FocusforProductivity...\n",
      "Saved 5660 1-star reviews to organized_data/Forest:FocusforProductivity/1_star_reviews.csv\n",
      "Saved 2956 2-star reviews to organized_data/Forest:FocusforProductivity/2_star_reviews.csv\n",
      "Saved 5969 3-star reviews to organized_data/Forest:FocusforProductivity/3_star_reviews.csv\n",
      "Saved 16975 4-star reviews to organized_data/Forest:FocusforProductivity/4_star_reviews.csv\n",
      "Saved 35216 5-star reviews to organized_data/Forest:FocusforProductivity/5_star_reviews.csv\n",
      "\n",
      "Processing Gratitude:Self-CareJournal...\n",
      "Saved 488 1-star reviews to organized_data/Gratitude:Self-CareJournal/1_star_reviews.csv\n",
      "Saved 235 2-star reviews to organized_data/Gratitude:Self-CareJournal/2_star_reviews.csv\n",
      "Saved 649 3-star reviews to organized_data/Gratitude:Self-CareJournal/3_star_reviews.csv\n",
      "Saved 2957 4-star reviews to organized_data/Gratitude:Self-CareJournal/4_star_reviews.csv\n",
      "Saved 14596 5-star reviews to organized_data/Gratitude:Self-CareJournal/5_star_reviews.csv\n",
      "\n",
      "Processing Habitica:GamifyYourTasks...\n",
      "Saved 908 1-star reviews to organized_data/Habitica:GamifyYourTasks/1_star_reviews.csv\n",
      "Saved 826 2-star reviews to organized_data/Habitica:GamifyYourTasks/2_star_reviews.csv\n",
      "Saved 1174 3-star reviews to organized_data/Habitica:GamifyYourTasks/3_star_reviews.csv\n",
      "Saved 1788 4-star reviews to organized_data/Habitica:GamifyYourTasks/4_star_reviews.csv\n",
      "Saved 2966 5-star reviews to organized_data/Habitica:GamifyYourTasks/5_star_reviews.csv\n",
      "\n",
      "Processing NCalendar-Simpleplanner...\n",
      "Saved 22 1-star reviews to organized_data/NCalendar-Simpleplanner/1_star_reviews.csv\n",
      "Saved 10 2-star reviews to organized_data/NCalendar-Simpleplanner/2_star_reviews.csv\n",
      "Saved 24 3-star reviews to organized_data/NCalendar-Simpleplanner/3_star_reviews.csv\n",
      "Saved 75 4-star reviews to organized_data/NCalendar-Simpleplanner/4_star_reviews.csv\n",
      "Saved 173 5-star reviews to organized_data/NCalendar-Simpleplanner/5_star_reviews.csv\n",
      "\n",
      "Processing Habitify:DailyHabitTracker...\n",
      "Saved 133 1-star reviews to organized_data/Habitify:DailyHabitTracker/1_star_reviews.csv\n",
      "Saved 98 2-star reviews to organized_data/Habitify:DailyHabitTracker/2_star_reviews.csv\n",
      "Saved 129 3-star reviews to organized_data/Habitify:DailyHabitTracker/3_star_reviews.csv\n",
      "Saved 175 4-star reviews to organized_data/Habitify:DailyHabitTracker/4_star_reviews.csv\n",
      "Saved 290 5-star reviews to organized_data/Habitify:DailyHabitTracker/5_star_reviews.csv\n",
      "\n",
      "Processing MoodTracker:Self-CareHabits...\n",
      "Saved 78 1-star reviews to organized_data/MoodTracker:Self-CareHabits/1_star_reviews.csv\n",
      "Saved 47 2-star reviews to organized_data/MoodTracker:Self-CareHabits/2_star_reviews.csv\n",
      "Saved 122 3-star reviews to organized_data/MoodTracker:Self-CareHabits/3_star_reviews.csv\n",
      "Saved 563 4-star reviews to organized_data/MoodTracker:Self-CareHabits/4_star_reviews.csv\n",
      "Saved 1347 5-star reviews to organized_data/MoodTracker:Self-CareHabits/5_star_reviews.csv\n",
      "\n",
      "Processing Engross:FocusTimer&To-Do...\n",
      "Saved 239 1-star reviews to organized_data/Engross:FocusTimer&To-Do/1_star_reviews.csv\n",
      "Saved 136 2-star reviews to organized_data/Engross:FocusTimer&To-Do/2_star_reviews.csv\n",
      "Saved 350 3-star reviews to organized_data/Engross:FocusTimer&To-Do/3_star_reviews.csv\n",
      "Saved 1046 4-star reviews to organized_data/Engross:FocusTimer&To-Do/4_star_reviews.csv\n",
      "Saved 1997 5-star reviews to organized_data/Engross:FocusTimer&To-Do/5_star_reviews.csv\n",
      "\n",
      "Processing Productive-Habittracker...\n",
      "Saved 580 1-star reviews to organized_data/Productive-Habittracker/1_star_reviews.csv\n",
      "Saved 217 2-star reviews to organized_data/Productive-Habittracker/2_star_reviews.csv\n",
      "Saved 319 3-star reviews to organized_data/Productive-Habittracker/3_star_reviews.csv\n",
      "Saved 418 4-star reviews to organized_data/Productive-Habittracker/4_star_reviews.csv\n",
      "Saved 709 5-star reviews to organized_data/Productive-Habittracker/5_star_reviews.csv\n",
      "\n",
      "Processing MinimalistPomodoroTimer...\n",
      "Saved 221 1-star reviews to organized_data/MinimalistPomodoroTimer/1_star_reviews.csv\n",
      "Saved 152 2-star reviews to organized_data/MinimalistPomodoroTimer/2_star_reviews.csv\n",
      "Saved 360 3-star reviews to organized_data/MinimalistPomodoroTimer/3_star_reviews.csv\n",
      "Saved 990 4-star reviews to organized_data/MinimalistPomodoroTimer/4_star_reviews.csv\n",
      "Saved 1982 5-star reviews to organized_data/MinimalistPomodoroTimer/5_star_reviews.csv\n",
      "\n",
      "Processing Me+DailyRoutinePlanner...\n",
      "Saved 3015 1-star reviews to organized_data/Me+DailyRoutinePlanner/1_star_reviews.csv\n",
      "Saved 1027 2-star reviews to organized_data/Me+DailyRoutinePlanner/2_star_reviews.csv\n",
      "Saved 1319 3-star reviews to organized_data/Me+DailyRoutinePlanner/3_star_reviews.csv\n",
      "Saved 3117 4-star reviews to organized_data/Me+DailyRoutinePlanner/4_star_reviews.csv\n",
      "Saved 7769 5-star reviews to organized_data/Me+DailyRoutinePlanner/5_star_reviews.csv\n",
      "\n",
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# import shutil\n",
    "\n",
    "\n",
    "def organize_reviews_by_sentiment(processed_data_dir=\"data/processed_data/play_store\"):\n",
    "    \"\"\"\n",
    "    Organize reviews into separate files based on sentiment scores for each app\n",
    "    \"\"\"\n",
    "    processed_dir = Path(processed_data_dir)\n",
    "    destination_dir = Path(\"organized_data\")\n",
    "    # Iterate through each app's CSV file\n",
    "    for csv_file in processed_dir.glob(\"*.csv\"):\n",
    "        app_name = csv_file.stem\n",
    "        print(f\"\\nProcessing {app_name}...\")\n",
    "\n",
    "        # Create app directory\n",
    "        app_dir = destination_dir / app_name\n",
    "        app_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Read the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "\n",
    "            # Create summary statistics\n",
    "            summary_stats = {\n",
    "                \"total_reviews\": len(df),\n",
    "                \"sentiment_distribution\": df[\"sentimentScore\"].value_counts().to_dict(),\n",
    "                \"average_sentiment\": df[\"sentimentScore\"].mean(),\n",
    "            }\n",
    "\n",
    "            # Split reviews by sentiment score and save to separate files\n",
    "            for sentiment in range(1, 6):\n",
    "                sentiment_df = df[df[\"sentimentScore\"] == sentiment]\n",
    "\n",
    "                if not sentiment_df.empty:\n",
    "                    # Save to sentiment-specific file\n",
    "                    output_file = app_dir / f\"{sentiment}_star_reviews.csv\"\n",
    "                    sentiment_df.to_csv(output_file, index=False)\n",
    "                    print(\n",
    "                        f\"Saved {len(sentiment_df)} {sentiment}-star reviews to {output_file}\"\n",
    "                    )\n",
    "\n",
    "            # Save summary statistics\n",
    "            summary_file = app_dir / \"sentiment_summary.txt\"\n",
    "            with open(summary_file, \"w\") as f:\n",
    "                f.write(f\"Sentiment Analysis Summary for {app_name}\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                f.write(f\"Total Reviews: {summary_stats['total_reviews']}\\n\\n\")\n",
    "                f.write(\"Sentiment Distribution:\\n\")\n",
    "                for stars, count in sorted(\n",
    "                    summary_stats[\"sentiment_distribution\"].items()\n",
    "                ):\n",
    "                    percentage = (count / summary_stats[\"total_reviews\"]) * 100\n",
    "                    f.write(f\"{stars} stars: {count} reviews ({percentage:.1f}%)\\n\")\n",
    "                f.write(\n",
    "                    f\"\\nAverage Sentiment Score: {summary_stats['average_sentiment']:.2f}\"\n",
    "                )\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {app_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting to organize reviews by sentiment...\")\n",
    "    organize_reviews_by_sentiment()\n",
    "    print(\"\\nProcess completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class SentimentFileHandler(FileSystemEventHandler):\n",
    "    def __init__(self, source_dir, output_base_dir):\n",
    "        self.source_dir = source_dir\n",
    "        self.output_base_dir = output_base_dir\n",
    "\n",
    "        # # Create output directories if they don't exist\n",
    "        # self.directories = {\n",
    "        #     'positive': os.path.join(output_base_dir, 'positive_sentiment'),\n",
    "        #     'negative': os.path.join(output_base_dir, 'negative_sentiment'),\n",
    "        #     'neutral': os.path.join(output_base_dir, 'neutral_sentiment'),\n",
    "        #     'processed': os.path.join(output_base_dir, 'processed_files'),\n",
    "        #     'logs': os.path.join(output_base_dir, 'logs')\n",
    "        # }\n",
    "\n",
    "        # for dir_path in self.directories.values():\n",
    "        #     os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory or not event.src_path.endswith(\".csv\"):\n",
    "            return\n",
    "\n",
    "        # Wait a short moment to ensure file is completely written\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            # Process the newly created file\n",
    "            self.organize_reviews_by_sentiment(event.src_path)\n",
    "        except Exception as e:\n",
    "            self.log_error(f\"Error processing {event.src_path}: {str(e)}\")\n",
    "\n",
    "    def organize_reviews_by_sentiment(self, file_path):\n",
    "        \"\"\"\n",
    "        Organize reviews into separate files based on sentiment scores for each app\n",
    "        \"\"\"\n",
    "        csv_file = Path(file_path)\n",
    "        destination_dir = Path(self.output_base_dir)\n",
    "        # Iterate through each app's CSV file\n",
    "        if csv_file.is_file:\n",
    "            app_name = csv_file.stem\n",
    "            print(f\"\\nProcessing {app_name}...\")\n",
    "\n",
    "            # Create app directory\n",
    "            app_dir = destination_dir / app_name\n",
    "            app_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Read the CSV file\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "\n",
    "                # Create summary statistics\n",
    "                summary_stats = {\n",
    "                    \"total_reviews\": len(df),\n",
    "                    \"sentiment_distribution\": df[\"sentimentScore\"]\n",
    "                    .value_counts()\n",
    "                    .to_dict(),\n",
    "                    \"average_sentiment\": df[\"sentimentScore\"].mean(),\n",
    "                }\n",
    "\n",
    "                # Split reviews by sentiment score and save to separate files\n",
    "                for sentiment in range(1, 6):\n",
    "                    sentiment_df = df[df[\"sentimentScore\"] == sentiment]\n",
    "\n",
    "                    if not sentiment_df.empty:\n",
    "                        # Save to sentiment-specific file\n",
    "                        output_file = app_dir / f\"{sentiment}_star_reviews.csv\"\n",
    "                        sentiment_df.to_csv(output_file, index=False)\n",
    "                        print(\n",
    "                            f\"Saved {len(sentiment_df)} {sentiment}-star reviews to {output_file}\"\n",
    "                        )\n",
    "\n",
    "                # Save summary statistics\n",
    "                summary_file = app_dir / \"sentiment_summary.txt\"\n",
    "                with open(summary_file, \"w\") as f:\n",
    "                    f.write(f\"Sentiment Analysis Summary for {app_name}\\n\")\n",
    "                    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                    f.write(f\"Total Reviews: {summary_stats['total_reviews']}\\n\\n\")\n",
    "                    f.write(\"Sentiment Distribution:\\n\")\n",
    "                    for stars, count in sorted(\n",
    "                        summary_stats[\"sentiment_distribution\"].items()\n",
    "                    ):\n",
    "                        percentage = (count / summary_stats[\"total_reviews\"]) * 100\n",
    "                        f.write(f\"{stars} stars: {count} reviews ({percentage:.1f}%)\\n\")\n",
    "                    f.write(\n",
    "                        f\"\\nAverage Sentiment Score: {summary_stats['average_sentiment']:.2f}\"\n",
    "                    )\n",
    "                    # Log successful processing\n",
    "                self.log_success(f\"Processed and organized {filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {app_name}: {str(e)}\")\n",
    "\n",
    "    # def process_sentiment_file(self, file_path):\n",
    "    #     try:\n",
    "    #         # Read the sentiment analysis results\n",
    "    #         df = pd.read_csv(file_path)\n",
    "\n",
    "    #         # Get the base filename\n",
    "    #         filename = os.path.basename(file_path)\n",
    "\n",
    "    #         # Determine overall sentiment (you may need to adjust this logic)\n",
    "    #         avg_sentiment = df[\"sentiment\"].mean() if \"sentiment\" in df.columns else 0\n",
    "\n",
    "    #         # Choose destination based on average sentiment\n",
    "    #         if avg_sentiment > 0.1:\n",
    "    #             dest_dir = self.directories[\"positive\"]\n",
    "    #         elif avg_sentiment < -0.1:\n",
    "    #             dest_dir = self.directories[\"negative\"]\n",
    "    #         else:\n",
    "    #             dest_dir = self.directories[\"neutral\"]\n",
    "\n",
    "    #         # Move file to appropriate directory\n",
    "    #         dest_path = os.path.join(dest_dir, filename)\n",
    "    #         shutil.move(file_path, dest_path)\n",
    "\n",
    "    #         # Create a copy in processed files directory\n",
    "    #         processed_path = os.path.join(self.directories[\"processed\"], filename)\n",
    "    #         shutil.copy2(dest_path, processed_path)\n",
    "\n",
    "    #         # Log successful processing\n",
    "    #         self.log_success(f\"Processed and organized {filename}\")\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         raise Exception(f\"Failed to process {file_path}: {str(e)}\")\n",
    "\n",
    "    def log_success(self, message):\n",
    "        self._log_message(\"SUCCESS\", message)\n",
    "\n",
    "    def log_error(self, message):\n",
    "        self._log_message(\"ERROR\", message)\n",
    "\n",
    "    def _log_message(self, level, message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file = os.path.join(self.directories[\"logs\"], \"file_organization.log\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"[{timestamp}] {level}: {message}\\n\")\n",
    "\n",
    "\n",
    "def start_file_monitoring(source_dir, output_base_dir):\n",
    "    \"\"\"\n",
    "    Start monitoring the source directory for new sentiment analysis files.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Directory to monitor for new files\n",
    "        output_base_dir (str): Base directory for organized output\n",
    "    \"\"\"\n",
    "    event_handler = SentimentFileHandler(source_dir, output_base_dir)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, source_dir, recursive=False)\n",
    "    observer.start()\n",
    "\n",
    "    print(f\"Started monitoring {source_dir}\")\n",
    "    print(f\"Organized files will be saved in {output_base_dir}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "        print(\"\\nStopped monitoring.\")\n",
    "\n",
    "    observer.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual directories\n",
    "    SOURCE_DIR = \"data/processed_data/play_store\"  # Directory where sentiment analysis saves results\n",
    "    OUTPUT_BASE_DIR = \"organized_results\"  # Base directory for organized files\n",
    "\n",
    "    start_file_monitoring(SOURCE_DIR, OUTPUT_BASE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
